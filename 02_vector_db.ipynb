{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Databases\n",
    "\n",
    "This notebook explains how to create text embeddings from documents, store them in a vector database, then use them to create context for an LLM like ChatGPT. It is based on tutorial at [RealPython](https://realpython.com/chromadb-vector-database) and Pere Martra's [LLM Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/2-Vector%20Databases%20with%20LLMs/how-to-use-a-embedding-database-with-a-llm-from-hf.ipynb) ([article](https://pub.towardsai.net/harness-the-power-of-vector-databases-influencing-language-models-with-personalized-information-ab2f995f09ba)).\n",
    "\n",
    "Embeddings are dense numeric vector encodings of unstructured objects like text, audio, and video. Vector databases like ChromaDB store embeddings. An application can parse a query to find the most relevant documents in the database, then include those documents as context in a message to the LLM. This is called retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings are the simplest application. A word emdedding is a many-dimensional vector of semantic relationships with other words. There are static word embedding algorithms (Word2vec, GloVe) and dynamic algorithms (like those used in LLMs) that change based on context. Distance functions measure vector similarity. Cosine similarity appears to be the preferred one (see [OpenAI Q&A](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"Compute the cosine similarity between two vectors\"\"\"\n",
    "    \n",
    "    return (u @ v) / (np.linalg.norm(u) * np.linalg.norm(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk creates a word embedding for a few words. The `en_core_web_md` model from the **spaCy** general purpose NLP library contains 20k 300-dimension word embeddings. Dog and cat have a hight similarity; dog and apple do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog/cat 0.8220817\n",
      "dog/apple 0.22881007\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Instantiate an embedding object with the medium-sized English model.\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Here are three embeddings from the model.\n",
    "dog_embedding = nlp.vocab[\"dog\"].vector\n",
    "cat_embedding = nlp.vocab[\"cat\"].vector\n",
    "apple_embedding = nlp.vocab[\"apple\"].vector\n",
    "\n",
    "# Use cosine similarity to meaure their similarity. Cat and dog are relatively similar compared to dog and apple.\n",
    "print(\"dog/cat\", cosine_similarity(dog_embedding, cat_embedding))\n",
    "print(\"dog/apple\", cosine_similarity(dog_embedding, apple_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "The logic of embeddings extends to sentences, documents, and even other data types such as audio and video. However, a simple model like `en_core_web_md` that is a dictionary of pre-calculated embeddings cannot embed text. Instead, use a pre-trained model that recognizes complex semantic relationships. The SentenceTransformers library works with multiple models, one of which is `all-MiniLM-L6-v2`. This model encodes texts up to 256 words, truncating anything longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.eurekalert.org/pub_releases/2020-0...</td>\n",
       "      <td>eurekalert.org</td>\n",
       "      <td>2020-08-06 13:59:45</td>\n",
       "      <td>A closer look at water-splitting's solar fuel ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.pulse.ng/news/world/an-irresistibl...</td>\n",
       "      <td>pulse.ng</td>\n",
       "      <td>2020-08-12 15:14:19</td>\n",
       "      <td>An irresistible scent makes locusts swarm, stu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.express.co.uk/news/science/1322607...</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>2020-08-13 21:01:00</td>\n",
       "      <td>Artificial intelligence warning: AI will know ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.ndtv.com/world-news/glaciers-could...</td>\n",
       "      <td>ndtv.com</td>\n",
       "      <td>2020-08-03 22:18:26</td>\n",
       "      <td>Glaciers Could Have Sculpted Mars Valleys: Study</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.thesun.ie/tech/5742187/perseid-met...</td>\n",
       "      <td>thesun.ie</td>\n",
       "      <td>2020-08-12 19:54:36</td>\n",
       "      <td>Perseid meteor shower 2020: What time and how ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                               link          domain  \\\n",
       "0  SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...  eurekalert.org   \n",
       "1  SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...        pulse.ng   \n",
       "2  SCIENCE  https://www.express.co.uk/news/science/1322607...   express.co.uk   \n",
       "3  SCIENCE  https://www.ndtv.com/world-news/glaciers-could...        ndtv.com   \n",
       "4  SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...       thesun.ie   \n",
       "\n",
       "        published_date                                              title lang  \n",
       "0  2020-08-06 13:59:45  A closer look at water-splitting's solar fuel ...   en  \n",
       "1  2020-08-12 15:14:19  An irresistible scent makes locusts swarm, stu...   en  \n",
       "2  2020-08-13 21:01:00  Artificial intelligence warning: AI will know ...   en  \n",
       "3  2020-08-03 22:18:26   Glaciers Could Have Sculpted Mars Valleys: Study   en  \n",
       "4  2020-08-12 19:54:36  Perseid meteor shower 2020: What time and how ...   en  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Got this from\n",
    "# https://www.kaggle.com/code/kerneler/starter-topic-labeled-news-dataset-870843f1-3\n",
    "news_data = pd.read_csv('./data/labelled_newscatcher_dataset.csv', sep=';')\n",
    "\n",
    "# Just keep 1,000 rows for demo\n",
    "news_subset = news_data.head(1000)\n",
    "\n",
    "news_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 384)\n",
      "NASA Releases In-Depth Map of Beirut Explosion Damage \n",
      "SpaceX, NASA Demo-2 Rocket Launch Set for Saturday: How to Watch \n",
      " 0.23386673629283905 \n",
      "\n",
      "SpaceX, NASA Demo-2 Rocket Launch Set for Saturday: How to Watch \n",
      "Orbital space tourism set for rebirth in 2021 \n",
      " 0.2754879593849182 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from cosine_similarity import compute_cosine_similarity\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode() creates a 384-dimension embedding for each title.\n",
    "text_embeddings = model.encode(news_subset[\"title\"])\n",
    "print(text_embeddings.shape)\n",
    "\n",
    "# Create a dictionary\n",
    "text_embeddings_dict = dict(zip(news_subset[\"title\"], list(text_embeddings)))\n",
    "\n",
    "# Try it out! Use cosine similarity to meaure their similarity.\n",
    "# Even though \"NASA\" is in both of the first two titles, the second two\n",
    "# are actually more similar. The text embedding picks up on that.\n",
    "\n",
    "test_1 = cosine_similarity(\n",
    "    text_embeddings_dict[news_subset[\"title\"][5]],\n",
    "    text_embeddings_dict[news_subset[\"title\"][6]]\n",
    ")\n",
    "print(f'{news_subset[\"title\"][5]} \\n{news_subset[\"title\"][6]} \\n {test_1} \\n')\n",
    "\n",
    "test_2 = cosine_similarity(\n",
    "    text_embeddings_dict[news_subset[\"title\"][6]],\n",
    "    text_embeddings_dict[news_subset[\"title\"][7]]\n",
    ")\n",
    "print(f'{news_subset[\"title\"][6]} \\n{news_subset[\"title\"][7]} \\n {test_2} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databases\n",
    "\n",
    "So far we've calculated embeddings for a collection of unstructured objects, stored them in a dictionary, then compared their similarity. That's great, but what you really want to do is find relevant matches to a search string. This is facilitated by storing the vectors in a database like ChromaDB. Let's create a database with 10 documents, each related to a different topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Define the db storage structure and create a directory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_data/\")\n",
    "\n",
    "# Set the embedding function\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Instantiate the storage structure. A collection is basically a database table.\n",
    "# Collection \"news_collection\" uses the all-MiniLM-L6-v2 embedding function and cosine \n",
    "# similarity.\n",
    "\n",
    "# Remove the existing one if present.\n",
    "if len(chroma_client.list_collections()) > 0 and \"news_collection\" in [chroma_client.list_collections()[0].name]:\n",
    "        chroma_client.delete_collection(name=\"news_collection\")\n",
    "\n",
    "chroma_collection = chroma_client.create_collection(\n",
    "    name=\"news_collection\",\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a database. Let's load it with 10 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection.add(\n",
    "    documents=news_subset[\"title\"].tolist(),\n",
    "    ids=[f\"id{i}\" for i in range(len(news_subset[\"title\"]))],\n",
    "    metadatas=[{\"TOPIC\": topic} for topic in news_subset[\"topic\"].tolist()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id7', 'id157', 'id122']],\n",
       " 'distances': [[0.46669214963912964, 0.5274306535720825, 0.563733696937561]],\n",
       " 'metadatas': [[{'TOPIC': 'SCIENCE'},\n",
       "   {'TOPIC': 'SCIENCE'},\n",
       "   {'TOPIC': 'SCIENCE'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Orbital space tourism set for rebirth in 2021',\n",
       "   'NASA astronauts \"This is an extraordinary day to be in space ...\" shoot music videos in the orbit',\n",
       "   'SpaceX brings NASA astronauts safely home']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results = chroma_collection.query(\n",
    "    query_texts=[\"I want to travel in space.\"],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id7', 'id157'], ['id710', 'id2']],\n",
       " 'distances': [[0.46669209003448486, 0.5274306535720825],\n",
       "  [0.4232991933822632, 0.4656652808189392]],\n",
       " 'metadatas': [[{'TOPIC': 'SCIENCE'}, {'TOPIC': 'SCIENCE'}],\n",
       "  [{'TOPIC': 'HEALTH'}, {'TOPIC': 'SCIENCE'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Orbital space tourism set for rebirth in 2021',\n",
       "   'NASA astronauts \"This is an extraordinary day to be in space ...\" shoot music videos in the orbit'],\n",
       "  ['The Ethics Of AI And Death',\n",
       "   'Artificial intelligence warning: AI will know us better than we know ourselves']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results = chroma_collection.query(\n",
    "    query_texts=[\"I want to travel in space.\",\n",
    "                 \"Is AI a threat?\"],\n",
    "    include=[\"documents\", \"distances\", \"metadatas\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# The documents [1] has two results, and documents [2] has two results. Reference\n",
    "# them as query_results[\"documents\"][0][1], query_results[\"documents\"][1][1], etc.\n",
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on the metadata so TOPIC is \"SCIENCE\". The \"HEALTH\" article is excluded now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id2', 'id198']],\n",
       " 'distances': [[0.46566540002822876, 0.5246715545654297]],\n",
       " 'metadatas': [[{'TOPIC': 'SCIENCE'}, {'TOPIC': 'SCIENCE'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Artificial intelligence warning: AI will know us better than we know ourselves',\n",
       "   'Scientists Discover New Material That Could ‘Merge AI With Human Brain’']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_collection.query(\n",
    "    query_texts=[\"Is AI a threat?\"],\n",
    "    where={\"TOPIC\": {\"$eq\": \"SCIENCE\"}},\n",
    "    n_results=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "Embeddings and vector databases are the foundation of retrieval-augmented generation (RAG) in LLMs. In RAG, documents are passed through an embedding function and loaded into the vector databse. Queries to the LLM is first passed through the embedding function and compared to the documents in the vector database. The most similar documents are sent as context to the LLM. RAG enables LLMs to make inferences using information that wasn't included in its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sending our data to OpenAI, this time we will download the [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b) model from Hugging Face. dolly-v2-3b is a 2.8 billion parameter model. pytoch_model.bin takes up 5.68G on my system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3184033cf64b16a01bc56872375597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpfol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mpfol\\.cache\\huggingface\\hub\\models--databricks--dolly-v2-3b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e006e7936964a058eca9fe00be44ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d1cda13bd54fe9b821df00cea29c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606bf45c4d4e4c84a134b230ca6be05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a2cfed48b74bc9aa9afa7f100139b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pipeline call, `max_new_tokens` limits the response size to 256 tokens. `device_map = \"autu\"`  lets the model decide whether to use CPU or GPU for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant context: #The Legendary Toshiba is Officially Done With Making Laptops #3 gaming laptop deals you can’t afford to miss today #Lenovo and HP control half of the global laptop market #Asus ROG Zephyrus G14 gaming laptop announced in India #Acer Swift 3 featuring a 10th-generation Intel Ice Lake CPU, 2K screen, and more launched in India for INR 64999 (US$865) #Apple's Next MacBook Could Be the Cheapest in Company's History #Features of Huawei's Desktop Computer Revealed #Redmi to launch its first gaming laptop on August 14: Here are all the details #Toshiba shuts the lid on laptops after 35 years #This is the cheapest Windows PC by a mile and it even has a spare SSD slot\n",
      "\n",
      " The user's question: Can I buy a Toshiba laptop?\n"
     ]
    }
   ],
   "source": [
    "# Let's get 10 article titles from our ChromaDB collection.\n",
    "results = chroma_collection.query(query_texts=[\"laptop\"], n_results=10 )\n",
    "\n",
    "# The prompt consistes of the context and question separated by a couple newlines.\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "question = \"Can I buy a Toshiba laptop?\"\n",
    "\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply pass the prompt into `pipe()`, wait about 90s, and the result appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant context: #The Legendary Toshiba is Officially Done With Making Laptops #3 gaming laptop deals you can’t afford to miss today #Lenovo and HP control half of the global laptop market #Asus ROG Zephyrus G14 gaming laptop announced in India #Acer Swift 3 featuring a 10th-generation Intel Ice Lake CPU, 2K screen, and more launched in India for INR 64999 (US$865) #Apple's Next MacBook Could Be the Cheapest in Company's History #Features of Huawei's Desktop Computer Revealed #Redmi to launch its first gaming laptop on August 14: Here are all the details #Toshiba shuts the lid on laptops after 35 years #This is the cheapest Windows PC by a mile and it even has a spare SSD slot\n",
      "\n",
      " The user's question: Can I buy a Toshiba laptop?\n",
      "The answer: No, Toshiba has decided to stop manufacturing laptops.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have passed this into OpenAI instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still find Toshiba laptops available for purchase from various retailers. However, please note that Toshiba has officially exited the laptop market, so the availability of new models may be limited. It is recommended to check with local retailers or online marketplaces to see if they have any Toshiba laptops in stock.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
