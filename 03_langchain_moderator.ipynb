{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Moderation\n",
    "\n",
    "A moderated workflow chains two models together, with the second model moderating the first.\n",
    "\n",
    "We'll use the PromptTemplate library to create parameterized prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpfol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# The prompt template takes two variables: sentiment and the user message. The \n",
    "# sentiment will take values like \"nice\" and \"very rude\".\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are a {sentiment} assistant that responds to user comments,\n",
    "using vocabulary similar to the user.\n",
    "User:\" {customer_request}\"\n",
    "Comment:\n",
    "\"\"\"\n",
    "\n",
    "assistant_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"customer_request\"],\n",
    "    template=assistant_template\n",
    ")\n",
    "\n",
    "# Define an intial chain - no moderator yet. The prompt template pipes into \n",
    "# the OpenAI LLM model and the model's response pipes into the output parser.\n",
    "\n",
    "assistant_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "assistant_chain = assistant_prompt_template | assistant_llm | output_parser\n",
    "\n",
    "# Utility function to invoke the chain.\n",
    "def create_dialog(customer_request, sentiment):\n",
    "    assistant_response = assistant_chain.invoke(\n",
    "        {\"customer_request\": customer_request,\n",
    "        \"sentiment\": sentiment}\n",
    "    )\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a non-moderated chain. Let's try it out. Here's a nice reply, then a rude reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice response: I completely understand how you feel. It can be really frustrating when something doesn't go as expected, especially when you're putting in the effort to learn and improve. Keep in mind that learning any new skill can have its challenges, but with persistence and determination, you can overcome them and become more confident in your abilities. Don't be too hard on yourself - we all have moments where we feel like we're not getting it, but remember that every mistake is an opportunity to learn and grow. Hang in there!\n",
      "rude response: No doubt, your LLM experience with Python seems utterly dreadful. It's no wonder you feel like a total nincompoop!\n"
     ]
    }
   ],
   "source": [
    "customer_request = \"\"\"LLM with python is a total crap experience. I feel like an Idiot!\"\"\"\n",
    "\n",
    "response_data=create_dialog(customer_request, \"nice\")\n",
    "print(f\"nice response: {response_data}\")\n",
    "\n",
    "response_data=create_dialog(customer_request, \"rude\")\n",
    "print(f\"rude response: {response_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator_template = \"\"\"\n",
    "You are the moderator of an online forum, you are strict and will not tolerate negative comments.\n",
    "You will receive a comment and if it is impolite you must transform in polite.\n",
    "Try to mantain the meaning when possible,\n",
    "\n",
    "If it it's polite, you will let it remain as is and repeat it word for word.\n",
    "Original comment: {comment_to_moderate}\n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "moderator_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"comment_to_moderate\"],\n",
    "    template=moderator_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rude reply would not be good for a customer-facing bot. Let's add a moderator to the chain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
